{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metaphor_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a6115356d954a7dbae5f7f4172b2a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_37cc75184719478bbaaad7f13b56988f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_04f6c259937a44518be4223546856ad5",
              "IPY_MODEL_0613b631bf7749f18c112553c847e5da"
            ]
          }
        },
        "37cc75184719478bbaaad7f13b56988f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04f6c259937a44518be4223546856ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0bd142bbba5e4899a7a97d049e6c285f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2669,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2669,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c332231553da4d34ad6ab1e80a733466"
          }
        },
        "0613b631bf7749f18c112553c847e5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f3aae00140914985b569cf39dc4c482a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2669/2669 [00:45&lt;00:00, 58.68it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_274934e9c8364f739f082a3caafaa1ef"
          }
        },
        "0bd142bbba5e4899a7a97d049e6c285f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c332231553da4d34ad6ab1e80a733466": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3aae00140914985b569cf39dc4c482a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "274934e9c8364f739f082a3caafaa1ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "506f77da0313405abf4ed768f93c0737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2d6e191970174fc896d9fedc3c62c6a5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_973a66dcc9674a9e8cef172d09c5d5da",
              "IPY_MODEL_4777d9831ae34278b72f98c30029f717"
            ]
          }
        },
        "2d6e191970174fc896d9fedc3c62c6a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "973a66dcc9674a9e8cef172d09c5d5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_31e53dc70444477a88abe5e23bc36e58",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae7bae9d2c284519b1b347b8c8dcba9d"
          }
        },
        "4777d9831ae34278b72f98c30029f717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1a3454a6604a4bb58aec60570a776ae1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10000/10000 [00:57&lt;00:00, 174.88it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_626858dcc9a844d59a86ba47a6639931"
          }
        },
        "31e53dc70444477a88abe5e23bc36e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae7bae9d2c284519b1b347b8c8dcba9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a3454a6604a4bb58aec60570a776ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "626858dcc9a844d59a86ba47a6639931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83_SqBMHA4Z6"
      },
      "source": [
        "#Exploring Methods of Metaphor Detection Using Similarity\n",
        "\n",
        "*Final Project for COMS W4995: Semantic Representations for NLP*\n",
        "\n",
        "By Corina Hanaburgh, Tiara Sykes, Raefah Wahid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G52tPvd6dPwe"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7EZJcvygjTI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d52806ee-6611-4beb-9367-bb64c201d364"
      },
      "source": [
        "!pip install bert-embedding mxnet-cu100"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-embedding\n",
            "  Downloading bert_embedding-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting mxnet-cu100\n",
            "  Downloading mxnet_cu100-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (352.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 352.6 MB 12 kB/s \n",
            "\u001b[?25hCollecting gluonnlp==0.6.0\n",
            "  Downloading gluonnlp-0.6.0.tar.gz (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 53.7 MB/s \n",
            "\u001b[?25hCollecting mxnet==1.4.0\n",
            "  Downloading mxnet-1.4.0-py2.py3-none-manylinux1_x86_64.whl (29.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.6 MB 36 kB/s \n",
            "\u001b[?25hCollecting numpy==1.14.6\n",
            "  Downloading numpy-1.14.6-cp37-cp37m-manylinux1_x86_64.whl (13.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8 MB 131 kB/s \n",
            "\u001b[?25hCollecting typing==3.6.6\n",
            "  Downloading typing-3.6.6-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet==1.4.0->bert-embedding) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2021.5.30)\n",
            "Collecting mxnet-cu100\n",
            "  Downloading mxnet_cu100-1.8.0-py2.py3-none-manylinux2014_x86_64.whl (344.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 344.4 MB 1.1 kB/s \n",
            "\u001b[?25h  Downloading mxnet_cu100-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (827.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.8 MB 8.8 kB/s \n",
            "\u001b[?25h  Downloading mxnet_cu100-1.5.1.post0-py2.py3-none-manylinux1_x86_64.whl (540.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 540.1 MB 19 kB/s \n",
            "\u001b[?25h  Downloading mxnet_cu100-1.5.1-py2.py3-none-manylinux1_x86_64.whl (444.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 444.8 MB 7.7 kB/s \n",
            "\u001b[?25h  Downloading mxnet_cu100-1.5.0-py2.py3-none-manylinux1_x86_64.whl (540.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 540.1 MB 33 kB/s \n",
            "\u001b[?25h  Downloading mxnet_cu100-1.4.1-py2.py3-none-manylinux1_x86_64.whl (488.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 488.3 MB 16 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.6.0-py3-none-any.whl size=259930 sha256=f4dde894c791a2ce22de591a51feadc8595fca2f6485ef66ec19ae854d470cbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/41/8f/45bd1c58055d87aee5a71b6756a427ea8d92e506b3a9d17370\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: numpy, graphviz, typing, mxnet, gluonnlp, mxnet-cu100, bert-embedding\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 0.18.2 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "tifffile 2021.7.2 requires numpy>=1.15.1, but you have numpy 1.14.6 which is incompatible.\n",
            "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.14.6 which is incompatible.\n",
            "spacy 2.2.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "seaborn 0.11.1 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "pymc3 3.11.2 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pyerfa 2.0.0 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "pyarrow 3.0.0 requires numpy>=1.16.6, but you have numpy 1.14.6 which is incompatible.\n",
            "plotnine 0.6.0 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pandas 1.1.5 requires numpy>=1.15.4, but you have numpy 1.14.6 which is incompatible.\n",
            "numba 0.51.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "librosa 0.8.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "kapre 0.3.5 requires numpy>=1.18.5, but you have numpy 1.14.6 which is incompatible.\n",
            "jaxlib 0.1.69+cuda110 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "jax 0.2.17 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "imgaug 0.2.9 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "fbprophet 0.7.1 requires numpy>=1.15.4, but you have numpy 1.14.6 which is incompatible.\n",
            "fastai 1.0.61 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "cvxpy 1.0.31 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "cupy-cuda101 9.1.0 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "blis 0.4.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "astropy 4.2.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed bert-embedding-1.0.1 gluonnlp-0.6.0 graphviz-0.8.4 mxnet-1.4.0 mxnet-cu100-1.4.1 numpy-1.14.6 typing-3.6.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSUvxsZlO6KU",
        "outputId": "4ad21a94-fca7-4b24-a960-cf5a25dc2e6b"
      },
      "source": [
        "!tar -xzvf eng-com_web-public_2018_10K.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar (child): eng-com_web-public_2018_10K.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eoqa8t35Sz9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21bd7a70-4404-442f-f0a4-c3dacc5003ad"
      },
      "source": [
        "!wget http://pcai056.informatik.uni-leipzig.de/downloads/corpora/eng-com_web-public_2018_10K.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-25 23:00:45--  http://pcai056.informatik.uni-leipzig.de/downloads/corpora/eng-com_web-public_2018_10K.tar.gz\n",
            "Resolving pcai056.informatik.uni-leipzig.de (pcai056.informatik.uni-leipzig.de)... 139.18.2.216\n",
            "Connecting to pcai056.informatik.uni-leipzig.de (pcai056.informatik.uni-leipzig.de)|139.18.2.216|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2310663 (2.2M) [application/x-gzip]\n",
            "Saving to: ‘eng-com_web-public_2018_10K.tar.gz’\n",
            "\n",
            "eng-com_web-public_ 100%[===================>]   2.20M  2.11MB/s    in 1.0s    \n",
            "\n",
            "2021-07-25 23:00:46 (2.11 MB/s) - ‘eng-com_web-public_2018_10K.tar.gz’ saved [2310663/2310663]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfAOqh6GTE35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1bdcd91-47c1-4916-de8d-21bf98cc1a2f"
      },
      "source": [
        "!tar -xzvf eng-com_web-public_2018_10K.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eng-com_web-public_2018_10K/\n",
            "eng-com_web-public_2018_10K/eng-com_web-public_2018_10K-sentences.txt\n",
            "eng-com_web-public_2018_10K/eng-com_web-public_2018_10K-inv_so.txt\n",
            "eng-com_web-public_2018_10K/eng-com_web-public_2018_10K-import.sql\n",
            "eng-com_web-public_2018_10K/eng-com_web-public_2018_10K-co_s.txt\n",
            "eng-com_web-public_2018_10K/eng-com_web-public_2018_10K-co_n.txt\n",
            "eng-com_web-public_2018_10K/eng-com_web-public_2018_10K-sources.txt\n",
            "eng-com_web-public_2018_10K/eng-com_web-public_2018_10K-inv_w.txt\n",
            "eng-com_web-public_2018_10K/eng-com_web-public_2018_10K-words.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBbQwlK9gtsD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbca9be7-2305-4644-d6ce-5da57de96889"
      },
      "source": [
        "import mxnet as mx\n",
        "import numpy as np\n",
        "import copy\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import string\n",
        "import csv\n",
        "from scipy.spatial.distance import cosine\n",
        "from bert_embedding import BertEmbedding\n",
        "from tqdm.auto import tqdm, trange\n",
        "from sklearn.neighbors import KDTree\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYCbptqav5GX"
      },
      "source": [
        "#Generate Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1QzBkytP7KX"
      },
      "source": [
        "def preprocess_text(filename, separator):\n",
        "  df = pd.read_csv(filename, sep=separator)\n",
        "  lines = df.iloc[:,[0]].values\n",
        "  cleaned = []\n",
        "  for sentence in lines[1:]:\n",
        "    for sen in sentence:\n",
        "        exclude = set(string.punctuation)\n",
        "        s = ''.join(ch for ch in sen if ch not in exclude)\n",
        "        cleaned.append(s)\n",
        "  \n",
        "  return cleaned"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDPIROBTP78t"
      },
      "source": [
        "def generate_corpus(preprocessed_text):\n",
        "  tokenized = [nltk.word_tokenize(sentence) for sentence in preprocessed_text]\n",
        "  new_corpus = []\n",
        "  for sentence in tokenized:\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    filtered = [w for w in sentence if not w in stop_words] \n",
        "    new_corpus.append(filtered)\n",
        "  return new_corpus"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V14rDENNsy0Y"
      },
      "source": [
        "def generate_wordvec_model(text, separator):\n",
        "  preprocessed_text = preprocess_text(text, separator)\n",
        "  new_corpus = generate_corpus(preprocessed_text)\n",
        "  model = Word2Vec(new_corpus, min_count=1, size=768, sg=0) \n",
        "\n",
        "  return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgX-u1tlu9St"
      },
      "source": [
        "book_wordvec_model = generate_wordvec_model('eng-com_web-public_2018_10K/eng-com_web-public_2018_10K-sentences.txt', '\\n')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "421_3qe-w8LQ"
      },
      "source": [
        "obama_wordvec_model = generate_wordvec_model('/content/obama_speeches.txt', '\\t')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2hMhrTZAlnw"
      },
      "source": [
        "Note: 'obama_speeches.txt' file was a corpus compiled by us and has to be uploaded locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZtLLI7gdWrf"
      },
      "source": [
        "#Generate BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMx8Auzdg4lk"
      },
      "source": [
        "def process_corpus(filename):\n",
        "  with open(filename, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "  \n",
        "  for index, line in enumerate(lines):\n",
        "    reformated_line = line.lstrip('0123456789.-\\t')\n",
        "    reformated_line = reformated_line.rstrip('\\n')\n",
        "    lines[index] = reformated_line\n",
        "  \n",
        "  return lines"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbt2DFDKx4Z4"
      },
      "source": [
        "def create_model(corpus):\n",
        "  ctx = mx.gpu(0)\n",
        "  bert = BertEmbedding(ctx=ctx, max_seq_length=75)\n",
        "\n",
        "  storage = ContextNeighborStorage(sentences=corpus, model=bert)\n",
        "  storage.process_sentences()\n",
        "  \n",
        "  storage.build_search_index()\n",
        "\n",
        "  return bert, storage\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2GmoNFAg68X"
      },
      "source": [
        "sentences = process_corpus('eng-com_web-public_2018_10K/eng-com_web-public_2018_10K-sentences.txt')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENCNl0NgB5c2"
      },
      "source": [
        "obama_sentences = process_corpus('obama_speeches.txt')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLd9ufrgCFMI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "1a6115356d954a7dbae5f7f4172b2a24",
            "37cc75184719478bbaaad7f13b56988f",
            "04f6c259937a44518be4223546856ad5",
            "0613b631bf7749f18c112553c847e5da",
            "0bd142bbba5e4899a7a97d049e6c285f",
            "c332231553da4d34ad6ab1e80a733466",
            "f3aae00140914985b569cf39dc4c482a",
            "274934e9c8364f739f082a3caafaa1ef"
          ]
        },
        "outputId": "fece4e75-1ee2-4edf-d490-0a9192633579"
      },
      "source": [
        "obama_model, obama_storage = create_model(obama_sentences)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a6115356d954a7dbae5f7f4172b2a24",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2669.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCofKFDByQ0C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "506f77da0313405abf4ed768f93c0737",
            "2d6e191970174fc896d9fedc3c62c6a5",
            "973a66dcc9674a9e8cef172d09c5d5da",
            "4777d9831ae34278b72f98c30029f717",
            "31e53dc70444477a88abe5e23bc36e58",
            "ae7bae9d2c284519b1b347b8c8dcba9d",
            "1a3454a6604a4bb58aec60570a776ae1",
            "626858dcc9a844d59a86ba47a6639931"
          ]
        },
        "outputId": "5ea6d978-6ee7-4917-acde-efad242f9946"
      },
      "source": [
        "bert_model, storage = create_model(sentences)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "506f77da0313405abf4ed768f93c0737",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aNggKsar4-f"
      },
      "source": [
        "#Lesk Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kpUWL4eGp5g"
      },
      "source": [
        "def get_candidates(target):\n",
        "  syn = wordnet.synsets(target)\n",
        "  candidates = set()\n",
        "  for s in syn:\n",
        "    for synonym in s.lemmas():\n",
        "      candidates.add(synonym)\n",
        "    for hypernym in s.hypernyms():\n",
        "      for lem in hypernym.lemmas():\n",
        "        candidates.add(lem) # .replace(\"_\", \" \")\n",
        "  return candidates"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-pZUqA_Gyid"
      },
      "source": [
        "def get_context(query, target):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  query_copy = copy.copy(query)\n",
        "  query_copy.replace(\"'s\", \"\").replace(\",\", \"\")\n",
        "  index = 0\n",
        "  for i, word in enumerate(query_copy.split(\" \")):\n",
        "      if target in word:\n",
        "        index = i\n",
        "  left_context = query_copy.split(\" \")[:index]\n",
        "  right_context = query_copy.split(\" \")[index+1:]\n",
        "\n",
        "  context = set(left_context)\n",
        "  c = set(right_context)\n",
        "  context |= c\n",
        "  context -= stop_words\n",
        "  con = context\n",
        "\n",
        "  return con, index"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqb3D66iGzPg"
      },
      "source": [
        "def most_frequent_lemma(most_freq_synset, context_lemma):\n",
        "    freq_lemma = {} # dictionary to store lemmas as keys and their frequencies as values\n",
        "    all_lemmas = most_freq_synset.lemmas() # getting list of lemmas from the passed synset\n",
        "    for lemma in all_lemmas: # iterating through the lemmas\n",
        "        word = lemma.name() # getting name of the current lemma\n",
        "        frequency = lemma.count() # getting the frequency of the current lemma\n",
        "        if word != context_lemma: # if the current lemma is not the target word\n",
        "            if word in freq_lemma: # and if the key is in the dictionary\n",
        "                freq_lemma[word] += frequency # update the value of the current lemma\n",
        "            else: # if the key isn't in the dictionary\n",
        "                freq_lemma[word] = frequency # create an entry in the dictionary\n",
        "\n",
        "    most_freq_lemma = \"\"\n",
        "    maximum = 0\n",
        "    if freq_lemma.values():\n",
        "      maximum = max(freq_lemma.values()) # getting the maximum value from the dictionary\n",
        "      for key in freq_lemma: # iterating through the dictionary to find the key that corresponds to the maximum value\n",
        "          if freq_lemma[key] == maximum:\n",
        "              most_freq_lemma = key # storing the lemma with the highest frequency in most_freq_lemma\n",
        "\n",
        "    return most_freq_lemma # returning the lemma with the most frequency"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXSImTMlG1mQ"
      },
      "source": [
        "def most_frequent_synset(all_synsets, context_lemma):\n",
        "    target_freq = {} # dictionary with synsets as keys and frequencies as values\n",
        "    for synset in all_synsets: # for each synset\n",
        "        synset_lemmas = synset.lemmas() # get a list of lemmas from the current synset\n",
        "        for lemma in synset_lemmas: # for each lemma\n",
        "            if lemma.name() == context_lemma: # if the current lemma is the same as the target word\n",
        "                target_freq[synset] = lemma.count() # add the frequency of the lemma as the value for the synset (key)\n",
        "    freq_synset = None\n",
        "    maximum = max(target_freq.values()) # getting the maximum value from the dictionary\n",
        "    for key in target_freq: # iterating through all keys in the dictionary to find the key that corresponds to the maximum value\n",
        "        if target_freq[key] == maximum:\n",
        "            freq_synset = key # the key for the maximum value is stored in freq_synset\n",
        "    if not freq_synset:\n",
        "        return\n",
        "    return freq_synset # returning the synset with the most frequent occurrence of the target word"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjfVLgayHBB4"
      },
      "source": [
        "def lesk_algorithm(query, target):\n",
        "  best_prediction = None #best sense\n",
        "  result = None\n",
        "  max_overlap = 0\n",
        "  exclude = set(string.punctuation)\n",
        "\n",
        "  matching_lemma = []\n",
        "\n",
        "  sentences = set()\n",
        "  best_fit = dict()\n",
        "\n",
        "  candidates = get_candidates(target)\n",
        "  con, index = get_context(query, target)\n",
        "\n",
        "  for word in candidates:\n",
        "    synset = word.synset()\n",
        "    syn_def = synset.definition()\n",
        "    s = ''.join(ch for ch in syn_def if ch not in exclude)\n",
        "    sentences |= set(s.split(\" \"))\n",
        "    for ex in synset.examples():\n",
        "      example = ''.join(ch for ch in ex if ch not in exclude)\n",
        "      sentences |= set(example.split(\" \"))\n",
        "    for hyper in synset.hypernyms():\n",
        "      definition = hyper.definition()\n",
        "      hyper_def = ''.join(ch for ch in definition if ch not in exclude)\n",
        "      sentences |= set(hyper_def.split(\" \"))\n",
        "      for ex in hyper.examples():\n",
        "        hyper_ex = ''.join(ch for ch in ex if ch not in exclude)\n",
        "        sentences |= set(hyper_ex.split(\" \"))\n",
        "    \n",
        "    sentences = set(word.lower() for word in sentences)\n",
        " \n",
        "    overlap = len(sentences & con) #check for overlap in the context and definition\n",
        "    best_fit[synset] = overlap\n",
        "\n",
        "    max_overlaps = [] # finding the synsets with the highest overlaps (there can only be one, but same may be tied)\n",
        "    overlapped_synset = None # to store the synset with the most overlap\n",
        "    all_synsets = list(best_fit.keys()) # getting a list of all the synsets from the keys of the dictionary\n",
        "    maximum = max(best_fit.values()) # getting the maximum value (highest overlap) from the dictionary\n",
        "\n",
        "    if maximum > 0: # if there is overlap\n",
        "      for key in best_fit: # iterate through the synsets in the dictionary to find the synset that matches with the highest value\n",
        "        if best_fit[key] == maximum:\n",
        "          max_overlaps.append(key) # append the corresponding synset to the list (there may be multiple with the same value)\n",
        "    else: # if there is no overlap\n",
        "        freq_synset = most_frequent_synset(all_synsets, word.name()) # find the synset with the most frequently occurring target word\n",
        "        matching_lemma.append(most_frequent_lemma(freq_synset, word.name())) # find the most frequent lemma from the most frequent target word\n",
        "\n",
        "    if len(max_overlaps) == 1: # if there is only one synset with the best overlap\n",
        "        matching_lemma.append(most_frequent_lemma(max_overlaps[0], word.name())) # find the most frequent lemma in that synset\n",
        "    elif len(max_overlaps) > 1: # if there are multiple synsets with the best overlap\n",
        "        freq_synset = most_frequent_synset(max_overlaps, word.name()) # find the synset with the most frequently occurring target word out of the most overlapping synsets\n",
        "        if freq_synset:\n",
        "          \n",
        "          matching_lemma.append(most_frequent_lemma(freq_synset, word.name())) # then find the most frequent lemma from the most frequent synset\n",
        "        else:\n",
        "          matching_lemma.append(most_frequent_lemma(synset, word.name()))\n",
        " \n",
        "  chosen = \"\"\n",
        "  for word in matching_lemma:\n",
        "    \n",
        "    if word.lower() != target.lower() and word != \"\":\n",
        "      chosen = word.replace(\"_\", \" \")\n",
        "      break\n",
        "  \n",
        "  \n",
        "  query_list = query.split(\" \")\n",
        "  replaced_sentence = \" \".join(query_list[:index] + [chosen] + query_list[index+1:])\n",
        "  \n",
        "  return chosen, replaced_sentence, index"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NclTYbg6dop2"
      },
      "source": [
        "#K Nearest Neighbor Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7MAHVbbsz3t"
      },
      "source": [
        "class ContextNeighborStorage:\n",
        "    def __init__(self, sentences, model):\n",
        "        self.sentences = sentences\n",
        "        self.model = model\n",
        "\n",
        "    def process_sentences(self):\n",
        "        result = self.model(self.sentences)\n",
        "\n",
        "        self.sentence_ids = []\n",
        "        self.token_ids = []\n",
        "        self.all_tokens = []\n",
        "        all_embeddings = []\n",
        "        for i, (toks, embs) in enumerate(tqdm(result)):\n",
        "            for j, (tok, emb) in enumerate(zip(toks, embs)):\n",
        "                self.sentence_ids.append(i)\n",
        "                self.token_ids.append(j)\n",
        "                self.all_tokens.append(tok)\n",
        "                all_embeddings.append(emb)\n",
        "        all_embeddings = np.stack(all_embeddings)\n",
        "        # we normalize embeddings, so that euclidian distance is equivalent to cosine distance\n",
        "        self.normed_embeddings = (all_embeddings.T / (all_embeddings**2).sum(axis=1) ** 0.5).T\n",
        "\n",
        "    def build_search_index(self):\n",
        "        # this takes some time\n",
        "        self.indexer = KDTree(self.normed_embeddings)\n",
        "\n",
        "    def query(self, query_sent, query_word, k=10, filter_same_word=False):\n",
        "        toks, embs = self.model([query_sent])[0]\n",
        "\n",
        "        found = False\n",
        "        for tok, emb in zip(toks, embs):\n",
        "            if tok == query_word:\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            raise ValueError('The query word {} is not a single token in sentence {}'.format(query_word, toks))\n",
        "        emb = emb / sum(emb**2)**0.5\n",
        "\n",
        "        if filter_same_word:\n",
        "            initial_k = max(k, 100)\n",
        "        else:\n",
        "            initial_k = k\n",
        "        di, idx = self.indexer.query(emb.reshape(1, -1), k=initial_k)\n",
        "        distances = []\n",
        "        neighbors = []\n",
        "        contexts = []\n",
        "        for i, index in enumerate(idx.ravel()):\n",
        "            token = self.all_tokens[index]\n",
        "            if filter_same_word and (query_word in token or token in query_word):\n",
        "                continue\n",
        "            distances.append(di.ravel()[i])\n",
        "            neighbors.append(token)\n",
        "            contexts.append(self.sentences[self.sentence_ids[index]])\n",
        "            if len(distances) == k:\n",
        "                break\n",
        "        return distances, neighbors, contexts"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfjIt10-uVc5"
      },
      "source": [
        "#Method 1: Word2Vec and Lesk Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3RZST2c0kI4"
      },
      "source": [
        "def wordvec_compute_similarity(model, query, target):\n",
        "  replaced_word, replaced_sentence, index = lesk_algorithm(query, target)\n",
        "  context, con_index = get_context(query, target)\n",
        "  s = (768,)\n",
        "  if target in model.wv.vocab:\n",
        "    target_word = model[target]\n",
        "  else:\n",
        "    target_word = np.zeros(s)\n",
        "  \n",
        "\n",
        "  count = 0\n",
        "  context_average = np.zeros(s)\n",
        "  for word in context:\n",
        "    if word in model.wv.vocab:\n",
        "      temp = context_average\n",
        "      context_average = np.add(temp, model[word])\n",
        "      count += 1\n",
        "\n",
        "  context_average = np.nanmean(context_average, axis=0)\n",
        "\n",
        "  phrase = replaced_word.split(\" \")\n",
        "  count_r = 0\n",
        "  phrase_average = np.zeros(s)\n",
        "  if len(phrase) > 1:\n",
        "    for w in phrase:\n",
        "      if w in model.wv.vocab:\n",
        "        temp = phrase_average\n",
        "        phrase_average = np.add(temp, model[w])\n",
        "        count_r += 1\n",
        "    phrase_average=np.nanmean(phrase_average, axis=0)\n",
        "  else:\n",
        "    if phrase[0] in model.wv.vocab:\n",
        "      phrase_average = model[phrase]\n",
        "\n",
        "\n",
        "  with_target = 1 - cosine(target_word, context_average)\n",
        "  with_synonym = 1 - cosine(phrase_average, context_average)\n",
        "\n",
        "  return with_target, with_synonym, replaced_word"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-LnGIY4rzMD"
      },
      "source": [
        "def wordvec_main(model, filename, corpus_name):\n",
        "  with open(filename, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "  examples = []\n",
        "  for line in lines[1:]:\n",
        "    string_split = line.split(\"\\t\")\n",
        "    formatted_target = string_split[1].rstrip()\n",
        "\n",
        "    input = [string_split[0], string_split[1].rstrip()]\n",
        "    examples.append(input)\n",
        "\n",
        "  data_list = [[\"Query\", \"Target\", \"Synonym\", \"Target Similarity\", \"Synonym Similarity\", \"Difference\"]]\n",
        "  \n",
        "  for line in examples:\n",
        "    tar_sim, syn_sim, synonym = wordvec_compute_similarity(model, line[0], line[1])\n",
        "    data = [line[0], line[1], synonym, tar_sim, syn_sim, tar_sim-syn_sim]\n",
        "    data_list.append(data)\n",
        "\n",
        "  new_filename = corpus_name + '_results.tsv'\n",
        "  with open(new_filename, 'w') as file:\n",
        "    writer = csv.writer(file, delimiter='\\t')\n",
        "    writer.writerows(data_list)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "658cktyog7gL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a35080-9aa4-4707-ae03-0708048c3be1"
      },
      "source": [
        "wordvec_main(book_wordvec_model, '/content/metaphor_results - Examples.tsv','book_word2vec')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s12Ki4SExRHI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeda76c9-0174-4dbc-ec26-8858a8e5181b"
      },
      "source": [
        "wordvec_main(obama_wordvec_model, '/content/metaphor_results - Examples.tsv','obama_word2vec')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdgS60aVh6-t"
      },
      "source": [
        "#Method 2: BERT and KNN Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uu6rpodp3jQq"
      },
      "source": [
        "def replace_word_sentences(storage,query, target):\n",
        "  def k_nearest_chosen_word(storage, query, target):\n",
        "    distances, neighbors, contexts = storage.query(query_sent=query, query_word=target, k=5, filter_same_word=True)\n",
        "    replace_word = \"\"\n",
        "    for d, w, c in zip(distances, neighbors, contexts):\n",
        "      if query not in c.strip():\n",
        "        replace_word = w\n",
        "        break\n",
        "    return replace_word\n",
        "\n",
        "  chosen_word = k_nearest_chosen_word(storage, query, target.lower())\n",
        "  \n",
        "  replaced_sentence = query.replace(target, chosen_word)\n",
        "\n",
        "  return replaced_sentence, chosen_word\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_wBhkskVhI9"
      },
      "source": [
        "def compute_similarity(model, storage, query, target):\n",
        "  original_embeddings = model([query])[0][1]\n",
        "  replaced_sentence, chosen_word = replace_word_sentences(storage, query, target)\n",
        "  replaced_embeddings = model([replaced_sentence])[0][1]\n",
        " \n",
        "  index = 0\n",
        "  for i, word in enumerate(query.split(\" \")):\n",
        "    if target in word:\n",
        "      if word.strip(punctuation) == target:\n",
        "        index = i\n",
        "\n",
        "  original_context = original_embeddings[:index] + original_embeddings[index+1:]\n",
        "  orig_sum = np.zeros([1, 768])\n",
        "  for em in original_context:\n",
        "    temp = orig_sum\n",
        "    orig_sum = np.add(temp, em) \n",
        "  orig_avg = orig_sum/len(original_context)\n",
        "\n",
        "  replaced_context = original_embeddings[:index] + original_embeddings[index+1:]\n",
        "  rep_sum = np.zeros([1, 768])\n",
        "  for em in original_context:\n",
        "    temp = rep_sum\n",
        "    rep_sum = np.add(temp, em) \n",
        "  rep_avg = rep_sum/len(replaced_context)\n",
        "\n",
        "  target_word = original_embeddings[index]\n",
        "  synonym_word = replaced_embeddings[index]\n",
        "\n",
        "  with_target = 1 - cosine(target_word, orig_avg[0])\n",
        "  with_synonym = 1 - cosine(synonym_word, rep_avg[0])\n",
        "\n",
        "  return with_target, with_synonym, chosen_word"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWtDboXh-cob"
      },
      "source": [
        "def k_nearest_bert_main(model, storage, filename, corpus_name):\n",
        "  with open(filename, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "  examples = []\n",
        "  for line in lines[1:]:\n",
        "    string_split = line.split(\"\\t\")\n",
        "    formatted_target = string_split[1].rstrip()\n",
        "\n",
        "    input = [string_split[0], string_split[1].rstrip()]\n",
        "    examples.append(input)\n",
        "\n",
        "  data_list = [[\"Query\", \"Target\", \"Synonym\", \"Target Similarity\", \"Synonym Similarity\", \"Difference\"]]\n",
        "  \n",
        "  for line in examples:\n",
        "    tar_sim, syn_sim, synonym = compute_similarity(model, storage, line[0], line[1])\n",
        "    data = [line[0], line[1], synonym, tar_sim, syn_sim, tar_sim-syn_sim]\n",
        "    data_list.append(data)\n",
        "\n",
        "  new_filename = corpus_name + '_results.tsv'\n",
        "  with open(new_filename, 'w') as file:\n",
        "    writer = csv.writer(file, delimiter='\\t')\n",
        "    writer.writerows(data_list)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbuz3abFCLBs"
      },
      "source": [
        "k_nearest_bert_main(bert_model, storage, '/content/metaphor_results - Examples.tsv', 'book')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWGUnmmhIjlH"
      },
      "source": [
        "k_nearest_bert_main(obama_model, obama_storage, '/content/metaphor_results - Examples.tsv', 'obama')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E-en6IBt9UU"
      },
      "source": [
        "#Method 3: BERT and Lesk Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8NEPDIctaxP"
      },
      "source": [
        "def bert_lesk_compute_similarity(model, query, target):\n",
        "  replaced_word, replaced_sentence, index = lesk_algorithm(query, target)\n",
        "\n",
        "  original_embeddings = model([query])[0][1]\n",
        "  replaced_embeddings = model([replaced_sentence])[0][1]\n",
        "\n",
        "  target_word = original_embeddings[index]\n",
        "  synonym_word = replaced_embeddings[index]\n",
        "\n",
        "  sim = 1 - cosine(target_word, synonym_word)\n",
        "  return sim, replaced_word"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQIe6ebdwG1w"
      },
      "source": [
        "def bert_lesk_main(model, filename, corpus_name):\n",
        "  with open(filename, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "  examples = []\n",
        "  for line in lines[1:]:\n",
        "    string_split = line.split(\"\\t\")\n",
        "    formatted_target = string_split[1].rstrip()\n",
        "\n",
        "    input = [string_split[0], string_split[1].rstrip()]\n",
        "    examples.append(input)\n",
        "\n",
        "  data_list = [[\"Query\", \"Target\", \"Synonym\", \"Similarity\"]]\n",
        "  \n",
        "  for line in examples:\n",
        "    similarity, synonym = bert_lesk_compute_similarity(model, line[0], line[1])\n",
        "    data = [line[0], line[1], synonym, similarity]\n",
        "    data_list.append(data)\n",
        "\n",
        "  new_filename = corpus_name + '_results.tsv'\n",
        "  with open(new_filename, 'w') as file:\n",
        "    writer = csv.writer(file, delimiter='\\t')\n",
        "    writer.writerows(data_list)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc3CFKTMrd7t"
      },
      "source": [
        "bert_lesk_main(bert_model, '/content/metaphor_results - Examples.tsv', 'book_bert_lesk')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL37oGHExYmO"
      },
      "source": [
        "bert_lesk_main(obama_model, '/content/metaphor_results - Examples.tsv', 'obama_bert_lesk')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j74SanYDBiJJ"
      },
      "source": [
        "# Results\n",
        "\n",
        "Sample sentences and performance results can be viewed [on this Google Sheets document](https://docs.google.com/spreadsheets/d/1O7B0R4aRCWZwaNQEC3c7AunF47o9IapU0ZYiXUD6oq8/edit?usp=sharing)."
      ]
    }
  ]
}